Proof of concept: The future of self-healing support in SuSE

Tags: devops, self-healing, orchestration

Solutions: Enterprise Linux, Mission-Critical Computing, 
           Cloud and as a Service Solution, Software-defined Storage

Links:
  https://www.mirantis.com/blog/auto-remediation-making-an-openstack-cloud-self-healing/
  https://code.facebook.com/posts/156810174519680/making-facebook-self-healing/
  https://www.dynatrace.com/blog/top-2-features-self-healing-microservices/


  "You've just deployed a new version of your application in your dev vms
   running SLES. After starting the performance tests you receive an email
   notification about a new bug filled in your Bugzilla project which reads: 
    
      TID 3597 - MCE detected in host nfs_vm1, host group BZ: NFS Cluster


   Made possible by OpenSUPPORT"


Glossary:

   SR   - Service request 
   DSRH - Default SR handler 
   ESRH - Escalation SR handler 
   CDR  - Call detail record
   SCC  - SuSE Customer Center
   RHCP - Red Hat Customer Portal





	   SuSE's Operations API - OpenSUPPORT


According to Dyntrace, the only barrier to self-healing is orchestration, to be 
more precise the lack of it or the troubleshooting overhead of having it.

You are sure there is no SPOF in a new LUN by having double controllers in both 
ends and by sparing disks for your RAID 5, so perhaps you could spare nodes in 
your data-center to wake from LAN and start spinning your end-points.

With orchestration, it is possible to feed a "circuit breaker" in case any
of your systems running SLES is having troubles. This moves your support to an 
automated system that deals with tickets and get you closer to problem 
resolution. Gone are the days a human is needed to open a ticket or upload 
support data.

The Operations API is the centerpiece of OpenSUPPORT, and allows your SLES
servers to open service requests and feed data to orchestration software so 
your system can self-heal. This can be done via remediation agents, meaning 
evacuation of a cluster host or shutdown of a system and provision of a new 
one. Think of it as an enabler of a Self-healing Infrastructure.

<img OpenSUPPORT.png>

The SR Router, upon receiving a CDR from the Operations API will generate 
a new ticket in your SR tool of choice. Based on your host group and the 
type of incident (detected by needles or manual escalation), it will route 
the SR to the appropriate system.

Then the Self-healing and orchestration layer is responsible for the logic 
behind disabling and enabling hosts, acting upon events and rules. A good 
example of such layer is StackStorm.

Let's say one of your systems encounter a memory problem. You might want to 
disable this system so a hardware team can run a health check. If this is part
of a cluster, the host can be evacuated at a time of your choice, and if this  
is an active/active instance in your OpenStack, the node can be shutdown. At 
the same time a ticket is being created in your Bugzilla/Jira/SCC, so the right 
people can troubleshoot the problem and release the server to be reused.

The OpenSUPPORT needles are the OS part of OpenSUPPORT. They run in your server
or in your VM as part of SLES and upon detecting a known problem they will:
  
  1) Create a CDR in /var/log/opensupport.log pointing you to the relevant TID

  2) Send the CDR to Operations API

     201707121630|vm1|172.16.32.1|mce_needle|tid:3467|critical|has_attachments

Attachments can be generated (support-tool) before CDR is submitted to the API.
You can even script what data you need to gather into a custom needle.

If no one acts upon the issue, you can escalate the issue by login in the
affected system and issuing:

  # opensupport escalate attachments

or

  # opensupport escalate

This will generate another CDR and in case you have a ESRH in your host group, 
it will escalate the SR to one or more ESRH. The first example will also 
attach support data to the handler so the team can troubleshoot the issue.

     201707121630|vm1|172.16.32.1|escalation|no_tid|critical|has_attachments

Alternatively you can escalate the SR via the Operations API Interface:

<img Operations_API_Interface.png>

Every host group should have a DSRH and alternatively many ESRH's:

    Unhandled host group
    Hosts: app1, app2, db1, db2
        DSRH: Jira - project MyEnterprise_nodes

    Host group #1
    Hosts: app_cluster1, app_cluster2
	DSRH: BZ - project MyAppDevNodes
	ESRH: SCC - user mycompany_level1
    
    Host group #2
    Hosts: mysql1, mysql2, mysql3, web1, web2, web3
        DSRH: SCC - user myproject_level1 
        ESRH: SCC - user myproject_level2 
        ESRH: SCC - user myproject_level3 

Integrating the Operations API in your infrastructure:
 
  * Spin the OpenSUPPORT container in HA cluster node and assign a public address
  * Add the Operations API IP to your DNS or /etc/hosts
  * Install OpenSUPPORT in your SLES and edit /etc/opensupport/config
    pointing to the Operations API hostname
  * Login to the Operations API interface and add a DSRH to the "unhandled host 
    group" and add the proper credentials. Upon registration hosts are
    automatically added to the unhandled host group.
  * You can also create personalized host groups with different handlers

Needles are test scripts that can detect common issues:

 - mce - machine check exception/hardware errors
 - oom - out of memory condition
 - hung tasks - blocked taks
 - soft/hard lockup
 - application segfault
 - io errors
 - page allocation failures
 - fork issues due to resource limitation
 - disk atime bigger than 100ms
 - load higher than the number of CPUs
 - call traces and panics
 - flapping interfaces
 - unexpected reboots
 - security upgrades
 - filesystem full
 - needles can be created by the Support Group

Facebook, LinkedIn, Netflix, and other hyper-scale operators use event-driven 
automation and workflows to automatically generate tickets, and they have built 
their own remediation layer to treat not only OS issues but also application
issues. Facebook's Dapper, the orchestration layer, has 5% of the tickets being 
generated by automation. Hundred's of man hours are saved by automating the 
process.

Enterprise Linux will continue to grow, and some problems can be dealt with
head-on approach. More novice admins will emerge, and leading customers to a 
more fault-tolerant infrastructure is paramount to keep a lean support 
organization.

  OpenSUPPORT might be the solution for these needs

These inbound "proactive" tickets should be dealt in bundles, by automating 
responses broke down into 3 layers. The first touch should be a comprehensive
message breaking down all the knowledge in small and simple parts. The next 
two layers should address more in-depth resolution to each particular problem.

The CDR is a fail-safe log in case orchestration and SR handlers are not in 
place. They can become notifications in Gnome or they could be redirected to
the console output as a mean to simplify Linux troubleshooting.

version 2

  * Needles: Guarantee 15 seconds execution, next round continue from where it stopped

########################################################################## Draft

Post study on "The future of self-healing support in SuSE"

In order to make things easier for customers to implement the Operations API, it
would be nice to have DNS auto discovery so agents can find the API and start
reporting. There are some service discovery hability in OpenStack as far as I 
could google.

It would be expected that every SLES the customers have are hooked up in a NMS
somehow, but this is not the case for the majority. Taking responsibility of 
SLES health ourselves will unburden customers from it and allow them to 
concentrate on their business. If a lean support organization is where we want
to be in the future, we should start telling when some situation is not ok. 
Would we be patronizing the customers? Is it best to expect them to do the right
thing and follow the best practises? Will our growth 

The CDRS will be issued again and again, e.g., every time the needles run and 
found your atime is > than 100ms, the CDR will be generated. 

Vaporware? What is gained with this system? The customer might not install the 
Operations API, thus no self-healing would be possible, nor tickets will be 
created, but they will receive alerts on the console which will give them 
reference to TIDs and a simple explanation of what might be a problem for their
system. The notifications will help them control their systems, not in an 
automated way, but will highlight and enlight them about how to take better 
advantage of their subscription. Take a look at your console log and you will
know automatically something is not wrong. 

The needles should be curated and created with attention. They should highlight 
scenarios that are not benefitial, with medium thresholds so heavy workloads 
have time to adapt. Enterprise storage should have atime with less than 10ms in 
an optimal workload. One would agree that having more than 100ms will start to 
create problems, even less than that. Likewise some systems will periodically 
report hung tasks, and for some developers that should not be a problem. A tape
device driver developer might say this is normal for instance. But nonetheless 
90% of the admins would agree hung tasks will be something to be avoided.

As the law of the vital few tell us, 80% of the effects come from 20% of the 
causes. The needles have to detect 20% of the causes, and cover 80% of the 
incidents. The other 20% of incidents shall be opened by our customers and 
will need humans to work on them. 


       Effects/Tickets        Causes           Taken care by
             
            80%                 20%             Automation


            20%                 80%          Technical Support



In a scenario where every customer is using the Operations API, we would see 80%
of the tickets being created by it. When we reach 4:1 ratio, meaning 4 tickets 
created by Operations API and 1 by customers, we would know it is working.

The ticket database (SCC) should provide what is the 20% causes which generate 
80% of the ticket

====== sandbox area =========

Commands:
	opensupport operations_interface         - start/stop web interface
	opensupport operations_api               - start/stop API
	opensupport sr_router                    - start/stop router
	opensupport self_healing_layer           - start/stop SH layer 
	opensupport needles                      - run needles
	opensupport sr --with-attachments
	opensupport escalate --with-attachments

How to run opensupport commands:
	memory        IO          CPU
	cgroups       ionice      last CPU
	64MB <2G      idle class
	128 >2 <16
	512MB > 16G 

        cgcreate -g memory:opensupport
        cgset memory
	cgexec -g memory:opensupport ionice -c idle taskset -c 11 opensupport needles

   Question? Will this make opensupport get blocked? If it does, will it
   generate a ticket?

When to run the needles
  * Every 3 hours from 7 till 19
  * Every hour from 20 till 7

  0 1 2 3 4 5 6    7 10 13 16 19   20 21 22 23

Todo:
  * capcom: Create support needles
  * capcom: Create Database layout
  * Write SR handlers (Bugzilla, SCC, Jira, TSANet, Red Hat, AWS, Azure, GCP)
  * Write SR Router and work the logic (DSRH, ESRH)
  * Write Operations API and Operations API Interface
  * Write Self-healing and Orchestration layer or integrate with StackStorm
     - cluster (host evacuation)
     - openstack and kubernetes (shutdown, spin)

Database schema

  host_group_table
  id | hostname | ip           | host group 
   0 | vm1      | 172.16.32.1  | 0
   1 | vm2      | 172.16.32.2  | 1

  group_handler_table
  id | group name | dsrh | esrh
   0 | default    | 0    |
   1 | my_group   | 1    | 2,3

  handler_type_config_table
  id | handler_name    | type
   0 | default handler | 0

  sr_cdr_table
  id | sr number | host | handler | summary                | execution time | 
   0 | 100301    | 0    | 2       | TID 3579: MCE detected | 5 seconds      |
 
  bz_handler_credentials_table
  id | sr_type | hostname          | username | password | project    | product
   0 | 0       | bugzilla.suse.com | user1    | pass1    | my_project |

  sr_types_table
  id | sr_types
   0 | bugzilla
   1 | suse
   2 | jira
   3 | redhat
   4 | tsanet
   5 | aws
   6 | azure
   7 | gcp


##### Capacity planning #####

OpenSUPPORT system requirements (arbritary/common sense):

	RAM:    64MB on systems with less than 2GB
		128MB on systems with less than 16GB and more than 2GB
		512MB on systems with more than 16GB of memory

	CPU:    1 CPU
	TIME:   15 minutes run, 16 times a day

        IO:     Idle class (might get blocked)

        DISK:   100MB for needle output archive
                 50MB for CDRs

     Network:   CDRs are 256 bytes long
                Handshake 

############# Needles prototype ###############

What does a needle do? It checks for some condition, and if it is true, it 
submits a cdr to Operations API.

The needle will save data temporarily in case the condition is true. 

Limit the size of the needle output directory to 100MB

Whatchdog 15 minutes of running time maximum.

# begin or continue needles execution
run_needles()
  global continue 

# registration
registration()

# Create Service request with operations API
submit_cdr()

# Escalate service request with ESRH's 
escalate()

# Check if process is already running so we can abort 
check_if_is_running()

# Exit the program with a error message in case a bad condition happens
abort()

# Maintain the cdr archive with less than 50MB
rotate_cdr_logs()

# The CDRs will not be marked as sent if Operations API timeout. If a CDR 
# times-out, there won't be a retry
timeout_submit_cdr()

